{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ab9e907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.linalg import qr\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda:4\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9fb90e",
   "metadata": {},
   "source": [
    "# visual function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c0321c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_F_list(F_list):\n",
    "\n",
    "    K = len(F_list)\n",
    "    plt.figure(figsize=(5 * K, 3))\n",
    "    for idx, f in enumerate(F_list):\n",
    "        plt.subplot(1, K, idx + 1)\n",
    "        plt.imshow(f.cpu(), cmap='coolwarm', interpolation='nearest')\n",
    "        plt.title(f\"F_{idx + 1}\")\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(\"Latent dim\")\n",
    "        plt.ylabel(\"Latent dim\")\n",
    "\n",
    "        p = f.shape[0]\n",
    "        ticks = list(range(p))\n",
    "        labels = list(range(1, p + 1))\n",
    "        plt.xticks(ticks, labels)\n",
    "        plt.yticks(ticks, labels)\n",
    "        \n",
    "    plt.suptitle(\"Sub-circuit dynamic matrices F_list\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visual_C(C):\n",
    "\n",
    "    plt.figure(figsize=(10,3))\n",
    "    for k in range(C.shape[0]):\n",
    "        plt.plot(C[k].cpu().numpy(), label=f'c_{k+1}')\n",
    "    plt.title(\"Sub-circuit coefficients C\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def visual_A(data):\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(data.detach().cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "    plt.colorbar(label='Projection weight')\n",
    "    plt.title(\"Projection matrix A\")\n",
    "    plt.xlabel(\"Latent dimension (p)\")\n",
    "    plt.ylabel(\"Neuron #\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visual_X(data):\n",
    "\n",
    "    X = data.detach().cpu().numpy()\n",
    "    plt.figure(figsize=(10,3))\n",
    "    for dim in range(X.shape[0]):\n",
    "        plt.plot(X[dim], label=f'x_{dim+1}')\n",
    "    plt.title(\"Latent dynamics X\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def visual_Y(data):\n",
    "    Y = data.detach().cpu().numpy()\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(Y, aspect='auto', origin='lower', cmap='viridis')\n",
    "    plt.colorbar(label='Firing rate')\n",
    "    plt.title(\"Synthetic observations Y\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Neuron #\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa98352",
   "metadata": {},
   "source": [
    "# optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdb5384",
   "metadata": {},
   "source": [
    "## a and x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bfed7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_h(Y):\n",
    "\n",
    "    Y_norm = Y / (Y.norm(dim=1, keepdim=True) + 1e-8)\n",
    "    h = Y_norm @ Y_norm.T\n",
    "    h = (h + 1.0) / 2.0\n",
    "    return h\n",
    "\n",
    "\n",
    "def similarity_loss(a, h):\n",
    "    D = torch.diag(h.sum(dim=1))\n",
    "    L = D - h\n",
    "    loss = torch.trace(a.T @ L @ a)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def update_a_and_x(Y, X, a, num_iter, lr_a, lr_x, lambda_sparse_A, lambda_sim, epoch):\n",
    "\n",
    "    N, T = Y.shape\n",
    "    p = X.shape[0]\n",
    "\n",
    "    h = compute_h(Y)  \n",
    "\n",
    "    A_var = a.clone().detach().requires_grad_(True)\n",
    "    X_var = X.clone().detach().requires_grad_(False)\n",
    "\n",
    "    optimizer_a = torch.optim.Adam([A_var], lr=lr_a)\n",
    "\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        Y_hat = A_var @ X_var   \n",
    "        loss_mse = F.mse_loss(Y, Y_hat)\n",
    "\n",
    "        loss_sparse = lambda_sparse_A * torch.norm(A_var, p=1, dim=0).mean()\n",
    "\n",
    "        loss_sim = lambda_sim * similarity_loss(A_var, h)\n",
    "\n",
    "        loss_a = loss_mse + loss_sparse + loss_sim\n",
    "\n",
    "        if epoch % 10 == 0 and i == 0:\n",
    "            print(f\"[Epoch {epoch}] \"\n",
    "                  f\"loss_rec_y={loss_mse.item():.6f}, \"\n",
    "                  f\"loss_sparse_a={loss_sparse.item():.6f}, \"\n",
    "                  f\"loss_sim_a={loss_sim.item():.6f}\")\n",
    "\n",
    "        optimizer_a.zero_grad()\n",
    "        loss_a.backward()\n",
    "        optimizer_a.step()\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            col_norms = torch.norm(A_var, p=2, dim=0, keepdim=True)\n",
    "            col_norms = torch.clamp(col_norms, min=1e-8)\n",
    "            A_var.div_(col_norms)\n",
    "    \n",
    "    next_a = A_var.detach()\n",
    "\n",
    "\n",
    "    A_var = next_a.clone().detach().requires_grad_(False)\n",
    "    X_var = X.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    optimizer_x = torch.optim.Adam([X_var], lr=lr_x)\n",
    "\n",
    "\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        Y_hat = A_var @ X_var\n",
    "        loss_rec_y = F.mse_loss(Y, Y_hat)\n",
    "\n",
    "\n",
    "        if epoch % 10 == 0 and i == 0:\n",
    "            print(f\"[Epoch {epoch}] loss_rec_y={loss_rec_y.item():.6f}\")\n",
    "\n",
    "        optimizer_x.zero_grad()\n",
    "        loss_rec_y.backward()\n",
    "        optimizer_x.step()\n",
    "\n",
    "    next_x = X_var.detach()\n",
    "\n",
    "    return next_a, next_x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2863dad3",
   "metadata": {},
   "source": [
    "## c and f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea99bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorrelation_loss(F_var: torch.Tensor, eps: float = 1e-8, reduction: str = \"sum\"):\n",
    "\n",
    "    K = F_var.shape[0]\n",
    "    flat = F_var.reshape(K, -1)  \n",
    "    flat = flat / (flat.norm(dim=1, keepdim=True).clamp_min(eps))\n",
    "\n",
    "    G = flat @ flat.t()\n",
    "\n",
    "    idx = torch.triu_indices(K, K, offset=1, device=F_var.device)\n",
    "    vals = (G[idx[0], idx[1]] ** 2)\n",
    "\n",
    "    return vals.sum()\n",
    "\n",
    "\n",
    "def update_c_and_f(X, C, F_list, num_iter, lr_c, lr_f, lambda_sparse_c, lambda_smooth_c, lambda_sparse_f, lambda_decor_f, epoch):\n",
    "\n",
    "    p, _ = X.shape\n",
    "    K, T = C.shape\n",
    "    \n",
    "    F_all = torch.stack(F_list, dim=0)\n",
    "\n",
    "    C_var = C.clone().detach().requires_grad_(True)\n",
    "    F_var = F_all.clone().detach().requires_grad_(False)\n",
    "\n",
    "    optimizer_c = torch.optim.Adam([C_var], lr=lr_c)\n",
    "\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        F_t_all = torch.einsum('kt,kij->tij', C_var, F_var)\n",
    "        X_t1_pred = torch.einsum('tij,jt->it', F_t_all[:-1, :, :], X[:, :-1])\n",
    "\n",
    "        loss_dyn_c = F.mse_loss(X[:, 1:], X_t1_pred)\n",
    "\n",
    "        loss_sparse_c = lambda_sparse_c * torch.norm(C_var, p=1, dim=0).mean()\n",
    "\n",
    "        diff = C_var[:, 1:] - C_var[:, :-1]\n",
    "        loss_smooth_c = lambda_smooth_c * diff.abs().mean()\n",
    "\n",
    "        loss_c = loss_dyn_c + loss_sparse_c + loss_smooth_c\n",
    "\n",
    "        optimizer_c.zero_grad()\n",
    "        loss_c.backward()\n",
    "        optimizer_c.step()\n",
    "\n",
    "        if epoch % 10 == 0 and i == 0:\n",
    "            print(f\"[Epoch {epoch}] loss_dyn_c={loss_dyn_c.item():.6f}, \"\n",
    "                  f\"loss_sparse_c={loss_sparse_c.item():.6f}, \"\n",
    "                  f\"loss_smooth_c={loss_smooth_c.item():.6f}\"\n",
    "                  )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            C_var.clamp_(min=0.0, max=1.0)\n",
    "\n",
    "            col_sums = C_var.sum(dim=0, keepdim=True)\n",
    "            col_sums = torch.clamp(col_sums, min=1e-8)\n",
    "            C_var.div_(col_sums)\n",
    "            \n",
    "    C_opt = C_var.detach()\n",
    "\n",
    "\n",
    "    C_var = C_opt.clone().detach().requires_grad_(False)\n",
    "    F_var = F_all.clone().detach().requires_grad_(True)\n",
    "\n",
    "    optimizer_f = torch.optim.Adam([F_var], lr=lr_f)\n",
    "\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        F_t_all = torch.einsum('kt,kij->tij', C_var, F_var)\n",
    "        X_t1_pred = torch.einsum('tij,jt->it', F_t_all[:-1, :, :], X[:, :-1])\n",
    "\n",
    "        loss_dyn_f = F.mse_loss(X[:, 1:], X_t1_pred)\n",
    "\n",
    "        loss_sparse_f = lambda_sparse_f * torch.norm(F_var, p=1)\n",
    "\n",
    "        loss_decor_f = lambda_decor_f * decorrelation_loss(F_var)\n",
    "\n",
    "        if epoch % 10 == 0 and i == 0:\n",
    "            print(f\"[Epoch {epoch}] loss_dyn_f={loss_dyn_f.item():.6f}, \"\n",
    "                  f\"loss_sparse_f={loss_sparse_f.item():.6f}, \"\n",
    "                  f\"loss_decor_f={loss_decor_f.item():.6f}\")\n",
    "\n",
    "        loss_f = loss_dyn_f + loss_sparse_f + loss_decor_f\n",
    "\n",
    "        optimizer_f.zero_grad()\n",
    "        loss_f.backward()\n",
    "        optimizer_f.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for k in range(K):\n",
    "                eigvals = torch.linalg.eigvals(F_var[k])\n",
    "                max_abs_eig = eigvals.abs().max()\n",
    "                F_var[k] /= max_abs_eig\n",
    "\n",
    "\n",
    "    F_opt = [F_var[i].detach() for i in range(K)]\n",
    "\n",
    "    return C_opt, F_opt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c3301",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ed014f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] loss_rec_y=1.110000, loss_sparse_a=0.053440, loss_sim_a=0.072397\n",
      "[Epoch 0] loss_rec_y=0.172586\n",
      "[Epoch 10] loss_rec_y=0.009870, loss_sparse_a=0.021588, loss_sim_a=0.004420\n",
      "[Epoch 10] loss_rec_y=0.009089\n",
      "[Epoch 20] loss_rec_y=0.003713, loss_sparse_a=0.021123, loss_sim_a=0.005753\n",
      "[Epoch 20] loss_rec_y=0.003592\n",
      "[Epoch 30] loss_rec_y=0.003246, loss_sparse_a=0.020441, loss_sim_a=0.007875\n",
      "[Epoch 30] loss_rec_y=0.003298\n",
      "[Epoch 40] loss_rec_y=0.003820, loss_sparse_a=0.018563, loss_sim_a=0.013238\n",
      "[Epoch 40] loss_rec_y=0.003924\n",
      "[Epoch 50] loss_rec_y=0.004466, loss_sparse_a=0.015873, loss_sim_a=0.019771\n",
      "[Epoch 50] loss_rec_y=0.004538\n",
      "[Epoch 60] loss_rec_y=0.004725, loss_sparse_a=0.013274, loss_sim_a=0.024673\n",
      "[Epoch 60] loss_rec_y=0.004761\n",
      "[Epoch 70] loss_rec_y=0.005084, loss_sparse_a=0.011715, loss_sim_a=0.027070\n",
      "[Epoch 70] loss_rec_y=0.005139\n",
      "[Epoch 80] loss_rec_y=0.005437, loss_sparse_a=0.011249, loss_sim_a=0.028146\n",
      "[Epoch 80] loss_rec_y=0.005473\n",
      "[Epoch 90] loss_rec_y=0.005874, loss_sparse_a=0.010846, loss_sim_a=0.029051\n",
      "[Epoch 90] loss_rec_y=0.005876\n",
      "[Epoch 100] loss_rec_y=0.005874, loss_sparse_a=0.010849, loss_sim_a=0.029039\n",
      "[Epoch 100] loss_rec_y=0.005869\n",
      "[Epoch 110] loss_rec_y=0.005874, loss_sparse_a=0.010847, loss_sim_a=0.029040\n",
      "[Epoch 110] loss_rec_y=0.005869\n",
      "[Epoch 120] loss_rec_y=0.005873, loss_sparse_a=0.010850, loss_sim_a=0.029037\n",
      "[Epoch 120] loss_rec_y=0.005869\n",
      "[Epoch 130] loss_rec_y=0.005873, loss_sparse_a=0.010852, loss_sim_a=0.029036\n",
      "[Epoch 130] loss_rec_y=0.005869\n",
      "[Epoch 140] loss_rec_y=0.005874, loss_sparse_a=0.010846, loss_sim_a=0.029041\n",
      "[Epoch 140] loss_rec_y=0.005869\n",
      "[Epoch 150] loss_rec_y=0.005874, loss_sparse_a=0.010850, loss_sim_a=0.029038\n",
      "[Epoch 150] loss_rec_y=0.005868\n",
      "[Epoch 160] loss_rec_y=0.005873, loss_sparse_a=0.010849, loss_sim_a=0.029038\n",
      "[Epoch 160] loss_rec_y=0.005868\n",
      "[Epoch 170] loss_rec_y=0.005873, loss_sparse_a=0.010851, loss_sim_a=0.029036\n",
      "[Epoch 170] loss_rec_y=0.005869\n",
      "[Epoch 180] loss_rec_y=0.005874, loss_sparse_a=0.010848, loss_sim_a=0.029039\n",
      "[Epoch 180] loss_rec_y=0.005869\n",
      "[Epoch 190] loss_rec_y=0.005874, loss_sparse_a=0.010848, loss_sim_a=0.029039\n",
      "[Epoch 190] loss_rec_y=0.005868\n",
      "[Epoch 0] loss_dyn_c=0.049787, loss_sparse_c=0.014952, loss_smooth_c=0.010214\n",
      "[Epoch 0] loss_dyn_f=0.001617, loss_sparse_f=0.002084, loss_decor_f=0.001407\n",
      "[Epoch 10] loss_dyn_c=0.000337, loss_sparse_c=0.010000, loss_smooth_c=0.001281\n",
      "[Epoch 10] loss_dyn_f=0.000347, loss_sparse_f=0.001955, loss_decor_f=0.001461\n",
      "[Epoch 20] loss_dyn_c=0.000228, loss_sparse_c=0.010000, loss_smooth_c=0.001309\n",
      "[Epoch 20] loss_dyn_f=0.000217, loss_sparse_f=0.001966, loss_decor_f=0.001474\n",
      "[Epoch 30] loss_dyn_c=0.000159, loss_sparse_c=0.010000, loss_smooth_c=0.001311\n",
      "[Epoch 30] loss_dyn_f=0.000157, loss_sparse_f=0.001964, loss_decor_f=0.001487\n",
      "[Epoch 40] loss_dyn_c=0.000164, loss_sparse_c=0.010000, loss_smooth_c=0.001262\n",
      "[Epoch 40] loss_dyn_f=0.000163, loss_sparse_f=0.001964, loss_decor_f=0.001488\n",
      "[Epoch 50] loss_dyn_c=0.000166, loss_sparse_c=0.010000, loss_smooth_c=0.001265\n",
      "[Epoch 50] loss_dyn_f=0.000168, loss_sparse_f=0.001960, loss_decor_f=0.001487\n",
      "[Epoch 60] loss_dyn_c=0.000170, loss_sparse_c=0.010000, loss_smooth_c=0.001267\n",
      "[Epoch 60] loss_dyn_f=0.000171, loss_sparse_f=0.001957, loss_decor_f=0.001487\n",
      "[Epoch 70] loss_dyn_c=0.000174, loss_sparse_c=0.010000, loss_smooth_c=0.001267\n",
      "[Epoch 70] loss_dyn_f=0.000174, loss_sparse_f=0.001955, loss_decor_f=0.001486\n",
      "[Epoch 80] loss_dyn_c=0.000177, loss_sparse_c=0.010000, loss_smooth_c=0.001268\n",
      "[Epoch 80] loss_dyn_f=0.000176, loss_sparse_f=0.001952, loss_decor_f=0.001486\n",
      "[Epoch 90] loss_dyn_c=0.000182, loss_sparse_c=0.010000, loss_smooth_c=0.001269\n",
      "[Epoch 90] loss_dyn_f=0.000181, loss_sparse_f=0.001949, loss_decor_f=0.001486\n",
      "[Epoch 100] loss_dyn_c=0.000180, loss_sparse_c=0.010000, loss_smooth_c=0.001271\n",
      "[Epoch 100] loss_dyn_f=0.000180, loss_sparse_f=0.001947, loss_decor_f=0.001485\n",
      "[Epoch 110] loss_dyn_c=0.000182, loss_sparse_c=0.010000, loss_smooth_c=0.001271\n",
      "[Epoch 110] loss_dyn_f=0.000183, loss_sparse_f=0.001946, loss_decor_f=0.001485\n",
      "[Epoch 120] loss_dyn_c=0.000181, loss_sparse_c=0.010000, loss_smooth_c=0.001272\n",
      "[Epoch 120] loss_dyn_f=0.000181, loss_sparse_f=0.001946, loss_decor_f=0.001485\n",
      "[Epoch 130] loss_dyn_c=0.000180, loss_sparse_c=0.010000, loss_smooth_c=0.001271\n",
      "[Epoch 130] loss_dyn_f=0.000181, loss_sparse_f=0.001948, loss_decor_f=0.001486\n",
      "[Epoch 140] loss_dyn_c=0.000184, loss_sparse_c=0.010000, loss_smooth_c=0.001270\n",
      "[Epoch 140] loss_dyn_f=0.000184, loss_sparse_f=0.001946, loss_decor_f=0.001486\n",
      "[Epoch 150] loss_dyn_c=0.000181, loss_sparse_c=0.010000, loss_smooth_c=0.001271\n",
      "[Epoch 150] loss_dyn_f=0.000181, loss_sparse_f=0.001947, loss_decor_f=0.001485\n",
      "[Epoch 160] loss_dyn_c=0.000184, loss_sparse_c=0.010000, loss_smooth_c=0.001271\n",
      "[Epoch 160] loss_dyn_f=0.000183, loss_sparse_f=0.001946, loss_decor_f=0.001485\n",
      "[Epoch 170] loss_dyn_c=0.000182, loss_sparse_c=0.010000, loss_smooth_c=0.001271\n",
      "[Epoch 170] loss_dyn_f=0.000182, loss_sparse_f=0.001947, loss_decor_f=0.001485\n",
      "[Epoch 180] loss_dyn_c=0.000180, loss_sparse_c=0.010000, loss_smooth_c=0.001271\n",
      "[Epoch 180] loss_dyn_f=0.000180, loss_sparse_f=0.001947, loss_decor_f=0.001485\n",
      "[Epoch 190] loss_dyn_c=0.000181, loss_sparse_c=0.010000, loss_smooth_c=0.001271\n",
      "[Epoch 190] loss_dyn_f=0.000182, loss_sparse_f=0.001947, loss_decor_f=0.001485\n",
      "[Epoch 0] loss_rec_y=0.005873, loss_sparse_a=0.000000, loss_sim_a=0.000000\n",
      "[Epoch 0] loss_rec_y=0.005771\n",
      "[Epoch 0] loss_dyn_c=0.000185, loss_sparse_c=0.010000, loss_smooth_c=0.001271\n",
      "[Epoch 0] loss_dyn_f=0.000185, loss_sparse_f=0.001948, loss_decor_f=0.001485\n",
      "[Epoch 10] loss_rec_y=0.004920, loss_sparse_a=0.000000, loss_sim_a=0.000000\n",
      "[Epoch 10] loss_rec_y=0.004833\n",
      "[Epoch 10] loss_dyn_c=0.000200, loss_sparse_c=0.010000, loss_smooth_c=0.001413\n",
      "[Epoch 10] loss_dyn_f=0.000201, loss_sparse_f=0.001945, loss_decor_f=0.001485\n",
      "[Epoch 20] loss_rec_y=0.004096, loss_sparse_a=0.000000, loss_sim_a=0.000000\n",
      "[Epoch 20] loss_rec_y=0.004018\n",
      "[Epoch 20] loss_dyn_c=0.000209, loss_sparse_c=0.010000, loss_smooth_c=0.001417\n",
      "[Epoch 20] loss_dyn_f=0.000209, loss_sparse_f=0.001948, loss_decor_f=0.001485\n",
      "[Epoch 30] loss_rec_y=0.003352, loss_sparse_a=0.000000, loss_sim_a=0.000000\n",
      "[Epoch 30] loss_rec_y=0.003282\n",
      "[Epoch 30] loss_dyn_c=0.000219, loss_sparse_c=0.010000, loss_smooth_c=0.001415\n",
      "[Epoch 30] loss_dyn_f=0.000219, loss_sparse_f=0.001949, loss_decor_f=0.001484\n",
      "[Epoch 40] loss_rec_y=0.002686, loss_sparse_a=0.000000, loss_sim_a=0.000000\n",
      "[Epoch 40] loss_rec_y=0.002624\n",
      "[Epoch 40] loss_dyn_c=0.000236, loss_sparse_c=0.010000, loss_smooth_c=0.001416\n",
      "[Epoch 40] loss_dyn_f=0.000236, loss_sparse_f=0.001960, loss_decor_f=0.001485\n"
     ]
    }
   ],
   "source": [
    "def main(F_list, X, C, Y, a, epoch_num, warmup_num):\n",
    "\n",
    "    for epoch in range(warmup_num):\n",
    "\n",
    "        a, X = update_a_and_x(Y, X, a, num_iter=20, lr_a=1e-3, lr_x=1e-2, lambda_sparse_A=0.005, lambda_sim=0.001, epoch=epoch)\n",
    "\n",
    "    for epoch in range(warmup_num):\n",
    "\n",
    "        C, F_list = update_c_and_f(X, C, F_list, num_iter=20, lr_c=1e-2, lr_f=1e-3, lambda_sparse_c=0.01, lambda_smooth_c=0.03, lambda_sparse_f=0.0002, lambda_decor_f=0.0005, epoch=epoch)\n",
    "            \n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        \n",
    "        a, X = update_a_and_x(Y, X, a, num_iter=2, lr_a=1e-3, lr_x=1e-2, lambda_sparse_A=0.0, lambda_sim=0.0, epoch=epoch)\n",
    "        C, F_list = update_c_and_f(X, C, F_list, num_iter=2, lr_c=1e-2, lr_f=1e-3, lambda_sparse_c=0.01, lambda_smooth_c=0.03, lambda_sparse_f=0.0002, lambda_decor_f=0.0005, epoch=epoch)\n",
    "\n",
    "    data = {\n",
    "        'C': C,\n",
    "        'X': X,\n",
    "        'A': a,\n",
    "        'Y': Y,\n",
    "        'F_list': F_list\n",
    "    }\n",
    "    return data\n",
    "\n",
    "N = 21\n",
    "p = 3\n",
    "K = 3\n",
    "T = 500\n",
    "\n",
    "torch.manual_seed(0)\n",
    "F_list = [torch.eye(p).to(device) + 0.1 * torch.randn(p, p).to(device) for _ in range(K)]\n",
    "C = torch.rand(K, T).to(device)  # [0, 1]\n",
    "a = torch.rand(N, p, requires_grad=False, device=device) # [0, 1]\n",
    "X = torch.randn(p, T, requires_grad=False, device=device) # [-3, 3]\n",
    "X[:, 0] = torch.tensor([1, -1, 1])\n",
    "\n",
    "\n",
    "data_true = torch.load(\"./data/Ten_Task_Synthetic_Data.pt\", weights_only=True)\n",
    "\n",
    "Y = data_true['Y']\n",
    "\n",
    "data_est = main(F_list, X, C, Y, a, epoch_num=50, warmup_num=200)\n",
    "\n",
    "# visual_F_list(data_est[\"F_list\"])\n",
    "# visual_C(data_est[\"C\"])\n",
    "# visual_A(data_est[\"A\"])\n",
    "# visual_X(data_est[\"X\"])\n",
    "# visual_Y(data_est[\"A\"] @ data_est[\"X\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3e1ebb",
   "metadata": {},
   "source": [
    "# compute MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1220a079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_Y': tensor(0.0021, device='cuda:4'), 'loss_X': tensor(0.0092, device='cuda:4'), 'loss_C': tensor(0.2945, device='cuda:4'), 'loss_F': tensor(0.0127, device='cuda:4'), 'loss_a': tensor(0.0081, device='cuda:4')}\n"
     ]
    }
   ],
   "source": [
    "def compute_losses(X, C, F_list, a, data_true):\n",
    "\n",
    "    true_X = data_true['X']\n",
    "    true_C = data_true['C']\n",
    "    true_F_list = data_true['F_list']\n",
    "    true_a = data_true['A_share']\n",
    "    true_Y = data_true['Y']\n",
    "\n",
    "    Y_hat = a @ X       # [N, T]\n",
    "    loss_Y = torch.mean((Y_hat - true_Y) ** 2)\n",
    "\n",
    "    loss_X = torch.mean((X - true_X) ** 2)\n",
    "\n",
    "    loss_C = torch.mean((C - true_C) ** 2)\n",
    "\n",
    "    loss_F = 0.0\n",
    "    for f_est, f_true in zip(F_list, true_F_list):\n",
    "        loss_F += torch.mean((f_est - f_true) ** 2)\n",
    "\n",
    "    loss_a = torch.mean((a - true_a) ** 2)\n",
    "\n",
    "    return {\n",
    "        'loss_Y': loss_Y,\n",
    "        'loss_X': loss_X,\n",
    "        'loss_C': loss_C,\n",
    "        'loss_F': loss_F,\n",
    "        'loss_a': loss_a\n",
    "    }\n",
    "\n",
    "re_F_list = data_est[\"F_list\"]\n",
    "re_C = data_est[\"C\"]\n",
    "re_A = data_est[\"A\"]\n",
    "re_X = data_est[\"X\"]\n",
    "re_Y = data_est[\"A\"] @ data_est[\"X\"]\n",
    "\n",
    "order = torch.tensor([1, 2, 0], device=re_A.device)\n",
    "re_A = re_A[:, order]\n",
    "re_X = re_X[order, :]\n",
    "\n",
    "order = [2, 1, 0]\n",
    "re_F_list = [re_F_list[i] for i in order]\n",
    "\n",
    "order = torch.tensor([2, 1, 0], device=re_A.device)\n",
    "re_C = re_C[order, :]\n",
    "\n",
    "results_mse = compute_losses(re_X, re_C, re_F_list, re_A, data_true)\n",
    "print(results_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc500731",
   "metadata": {},
   "source": [
    "# compute_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7788c338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corr_A': 0.8874708859142091, 'corr_F_list': [0.9894045319707607, 0.9850613376873179, 0.9974793646161484], 'corr_C': -0.09897517666534374, 'corr_X': 0.9677740897273229, 'corr_Y': 0.9478141932684903}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def p(X, C, F_list, a, data_true):\n",
    "\n",
    "    def corrcoef(a, b):\n",
    "        a_flat = a.flatten()\n",
    "        b_flat = b.flatten()\n",
    "        return np.corrcoef(a_flat, b_flat)[0, 1]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    Y_hat = a @ X\n",
    "\n",
    "    A_true = data_true['A_share'].cpu().numpy()\n",
    "    A_est = a.cpu().numpy()\n",
    "    results['corr_A'] = corrcoef(A_true, A_est)\n",
    "\n",
    "    corr_F = []\n",
    "    F_true_list = data_true['F_list']\n",
    "    F_est_list = F_list\n",
    "    for f_true, f_est in zip(F_true_list, F_est_list):\n",
    "        f_true = f_true.cpu().numpy()\n",
    "        f_est = f_est.cpu().numpy()\n",
    "        corr = corrcoef(f_true, f_est)\n",
    "        corr_F.append(corr)\n",
    "    results['corr_F_list'] = corr_F\n",
    "\n",
    "    C_true = data_true['C'].cpu().numpy()\n",
    "    C_est = C.cpu().numpy()\n",
    "    results['corr_C'] = corrcoef(C_true, C_est)\n",
    "\n",
    "    X_true = data_true['X'].cpu().numpy()\n",
    "    X_est = X.cpu().numpy()\n",
    "    results['corr_X'] = corrcoef(X_true, X_est)\n",
    "\n",
    "    Y_true = data_true['Y'].cpu().numpy()\n",
    "    Y_est = Y_hat.cpu().numpy()\n",
    "    results['corr_Y'] = corrcoef(Y_true, Y_est)\n",
    "\n",
    "    return results\n",
    "\n",
    "results_p = p(re_X, re_C, re_F_list, re_A, data_true)\n",
    "print(results_p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
